{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Assignment 2 - Heart Disease Classification",
        "## Student: Raji",
        "## Date: February 15, 2026"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd",
        "import numpy as np",
        "",
        "from sklearn.model_selection import train_test_split",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder",
        "from sklearn.impute import SimpleImputer",
        "from sklearn.metrics import (",
        "    accuracy_score, roc_auc_score, precision_score,",
        "    recall_score, f1_score, matthews_corrcoef,",
        "    confusion_matrix, classification_report",
        ")",
        "",
        "from sklearn.linear_model import LogisticRegression",
        "from sklearn.tree import DecisionTreeClassifier",
        "from sklearn.neighbors import KNeighborsClassifier",
        "from sklearn.naive_bayes import GaussianNB",
        "from sklearn.ensemble import RandomForestClassifier",
        "from xgboost import XGBClassifier",
        "",
        "import joblib",
        "import os",
        "",
        "import warnings",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the heart disease dataset",
        "df = pd.read_csv(\"heart_disease_uci.csv\")",
        "print(\"Dataset Shape:\", df.shape)",
        "print(\"\\nFirst few rows:\")",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check columns and data types",
        "print(\"Columns:\", df.columns.tolist())",
        "print(\"\\nData types:\\n\", df.dtypes)",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())",
        "print(\"\\nTarget distribution:\\n\", df[\"num\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing",
        "### 4.1 Separate Features and Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target",
        "X = df.drop(\"num\", axis=1)",
        "y = df[\"num\"]",
        "",
        "# Convert to binary classification (0: No disease, 1: Disease)",
        "y = (y > 0).astype(int)",
        "",
        "print(\"Feature shape:\", X.shape)",
        "print(\"Target shape:\", y.shape)",
        "print(\"Target classes:\", y.unique())",
        "print(\"Target distribution:\\n\", y.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Encode Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode categorical variables",
        "X_encoded = X.copy()",
        "label_encoders = {}",
        "",
        "for col in X_encoded.columns:",
        "    if X_encoded[col].dtype == \"object\":",
        "        le = LabelEncoder()",
        "        X_encoded[col] = le.fit_transform(X_encoded[col])",
        "        label_encoders[col] = le",
        "        print(f\"Encoded column: {col}\")",
        "",
        "print(f\"\\nTotal categorical columns encoded: {len(label_encoders)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets",
        "X_train, X_test, y_train, y_test = train_test_split(",
        "    X_encoded, y,",
        "    test_size=0.2,",
        "    random_state=42,",
        "    stratify=y",
        ")",
        "",
        "print(\"Training set size:\", X_train.shape)",
        "print(\"Test set size:\", X_test.shape)",
        "print(\"\\nTrain target distribution:\\n\", y_train.value_counts())",
        "print(\"\\nTest target distribution:\\n\", y_test.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features using StandardScaler",
        "scaler = StandardScaler()",
        "",
        "# Fit on training data and transform both train and test",
        "X_train_scaled = scaler.fit_transform(X_train)",
        "X_test_scaled = scaler.transform(X_test)",
        "",
        "print(\"Training data scaled\")",
        "print(\"Test data transformed using training scaler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Impute missing values using median strategy",
        "imputer = SimpleImputer(strategy=\"median\")",
        "",
        "# Fit on training data and transform both train and test",
        "X_train_final = imputer.fit_transform(X_train_scaled)",
        "X_test_final = imputer.transform(X_test_scaled)",
        "",
        "print(\"NaNs in training data:\", np.isnan(X_train_final).sum())",
        "print(\"NaNs in test data:\", np.isnan(X_test_final).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 Save Preprocessing Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model directory",
        "os.makedirs(\"model\", exist_ok=True)",
        "",
        "# Save preprocessing objects for deployment",
        "joblib.dump(imputer, \"model/imputer.pkl\")",
        "joblib.dump(scaler, \"model/scaler.pkl\")",
        "",
        "print(\"\u2713 Imputer saved: model/imputer.pkl\")",
        "print(\"\u2713 Scaler saved: model/scaler.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training and Evaluation",
        "### 5.1 Define Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define all classification models",
        "models = {",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),",
        "    \"Naive Bayes\": GaussianNB(),",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),",
        "    \"XGBoost\": XGBClassifier(",
        "        use_label_encoder=False,",
        "        eval_metric=\"logloss\",",
        "        random_state=42",
        "    )",
        "}",
        "",
        "print(\"Models defined:\")",
        "for name in models.keys():",
        "    print(f\"  \u2022 {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Train Models and Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models and evaluate",
        "results = []",
        "",
        "for name, model in models.items():",
        "    print(f\"\\nTraining {name}...\")",
        "    ",
        "    # Train model",
        "    model.fit(X_train_final, y_train)",
        "    ",
        "    # Make predictions",
        "    y_pred = model.predict(X_test_final)",
        "    y_prob = model.predict_proba(X_test_final)[:, 1]",
        "    ",
        "    # Calculate metrics",
        "    metrics = {",
        "        \"Model\": name,",
        "        \"Accuracy\": round(accuracy_score(y_test, y_pred), 4),",
        "        \"AUC\": round(roc_auc_score(y_test, y_prob), 4),",
        "        \"Precision\": round(precision_score(y_test, y_pred), 4),",
        "        \"Recall\": round(recall_score(y_test, y_pred), 4),",
        "        \"F1\": round(f1_score(y_test, y_pred), 4),",
        "        \"MCC\": round(matthews_corrcoef(y_test, y_pred), 4)",
        "    }",
        "    ",
        "    results.append(metrics)",
        "    ",
        "    # Save model",
        "    model_filename = f\"model/{name.replace(' ', '_')}.pkl\"",
        "    joblib.dump(model, model_filename)",
        "    print(f\"\u2713 Model saved: {model_filename}\")",
        "    ",
        "    # Print metrics",
        "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")",
        "    print(f\"  AUC: {metrics['AUC']:.4f}\")",
        "    print(f\"  F1 Score: {metrics['F1']:.4f}\")",
        "",
        "print(\"\\n\" + \"=\"*50)",
        "print(\"All models trained and saved successfully!\")",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results DataFrame",
        "results_df = pd.DataFrame(results)",
        "",
        "# Display results",
        "print(\"\\nMODEL COMPARISON TABLE\")",
        "print(\"=\"*80)",
        "print(results_df.to_string(index=False))",
        "print(\"=\"*80)",
        "",
        "# Find best model for each metric",
        "print(\"\\nBEST MODELS:\")",
        "for metric in [\"Accuracy\", \"AUC\", \"Precision\", \"Recall\", \"F1\", \"MCC\"]:",
        "    best_idx = results_df[metric].idxmax()",
        "    best_model = results_df.loc[best_idx, \"Model\"]",
        "    best_score = results_df.loc[best_idx, metric]",
        "    print(f\"  {metric:12s}: {best_model:20s} ({best_score:.4f})\")",
        "",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Save Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save metrics for documentation",
        "results_df.to_csv(\"model/model_metrics.csv\", index=False)",
        "print(\"\u2713 Metrics saved: model/model_metrics.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Prepare Test Data for Streamlit",
        "### 6.1 Save Preprocessed Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert preprocessed test data back to DataFrame",
        "X_test_df = pd.DataFrame(X_test_final, columns=X_encoded.columns)",
        "",
        "# Add target column",
        "X_test_df[\"num\"] = y_test.values",
        "",
        "# Save test data",
        "X_test_df.to_csv(\"test_data.csv\", index=False)",
        "",
        "print(\"\u2713 Test data saved: test_data.csv\")",
        "print(f\"  Shape: {X_test_df.shape}\")",
        "print(f\"\\nFirst few rows:\")",
        "print(X_test_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Verification",
        "### 7.1 Load and Test Saved Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that saved models work correctly",
        "print(\"VERIFYING SAVED MODELS\\n\" + \"=\"*50)",
        "",
        "for name in models.keys():",
        "    model_filename = f\"model/{name.replace(' ', '_')}.pkl\"",
        "    ",
        "    # Load model",
        "    loaded_model = joblib.load(model_filename)",
        "    ",
        "    # Make predictions",
        "    y_pred = loaded_model.predict(X_test_final)",
        "    accuracy = accuracy_score(y_test, y_pred)",
        "    ",
        "    print(f\"{name:20s}: Accuracy = {accuracy:.4f} \u2713\")",
        "",
        "print(\"\\n\" + \"=\"*50)",
        "print(\"All models loaded and verified successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary",
        "",
        "### Files Created:",
        "- **model/Logistic_Regression.pkl** - Trained Logistic Regression model",
        "- **model/Decision_Tree.pkl** - Trained Decision Tree model",
        "- **model/KNN.pkl** - Trained KNN model",
        "- **model/Naive_Bayes.pkl** - Trained Naive Bayes model",
        "- **model/Random_Forest.pkl** - Trained Random Forest model",
        "- **model/XGBoost.pkl** - Trained XGBoost model",
        "- **model/imputer.pkl** - Fitted SimpleImputer",
        "- **model/scaler.pkl** - Fitted StandardScaler",
        "- **model/model_metrics.csv** - All model metrics",
        "- **test_data.csv** - Preprocessed test data for Streamlit",
        "",
        "### Next Steps:",
        "1. Upload all pkl files to GitHub in a `model/` directory",
        "2. Upload test_data.csv to GitHub",
        "3. Deploy Streamlit app using app.py",
        "4. Test the app with test_data.csv"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}